[2019-08-28 07:39:53,000 INFO] Device ID 0
[2019-08-28 07:39:53,000 INFO] Device cuda
[2019-08-28 07:39:59,862 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json not found in cache, downloading to /tmp/tmppnzw8_nl
[2019-08-28 07:40:00,262 INFO] copying /tmp/tmppnzw8_nl to cache at ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 07:40:00,263 INFO] creating metadata file for ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 07:40:00,263 INFO] removing temp file /tmp/tmppnzw8_nl
[2019-08-28 07:40:00,264 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 07:40:00,264 INFO] Model config {
  "attn_type": "bi",
  "bi_data": false,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "n_head": 12,
  "n_layer": 12,
  "n_token": 32000,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "torchscript": false,
  "untie_r": true
}

[2019-08-28 07:40:00,625 INFO] https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin not found in cache, downloading to /tmp/tmp6zxcir8k
[2019-08-28 07:44:50,243 INFO] copying /tmp/tmp6zxcir8k to cache at ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 07:44:50,805 INFO] creating metadata file for ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 07:44:50,806 INFO] removing temp file /tmp/tmp6zxcir8k
[2019-08-28 07:44:50,867 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 07:44:56,292 INFO] Summarizer(
  (xlnet): XLNet(
    (model): XLNetModel(
      (word_embedding): Embedding(32000, 768)
      (layer): ModuleList(
        (0): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (6): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (7): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (8): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (9): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (10): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (11): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layer): Classifier(
    (linear1): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2019-08-28 07:44:56,296 INFO] * number of parameters: 116719105
[2019-08-28 07:44:56,296 INFO] Start training...
[2019-08-28 07:44:56,437 INFO] Loading train dataset from ../xlnet_data/cnndm.train.51.xlnet.pt, number of examples: 1999
[2019-08-28 07:49:32,763 INFO] Device ID 0
[2019-08-28 07:49:32,763 INFO] Device cuda
[2019-08-28 07:49:38,823 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 07:49:38,823 INFO] Model config {
  "attn_type": "bi",
  "bi_data": false,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "n_head": 12,
  "n_layer": 12,
  "n_token": 32000,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "torchscript": false,
  "untie_r": true
}

[2019-08-28 07:49:39,184 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 07:49:44,661 INFO] Summarizer(
  (xlnet): XLNet(
    (model): XLNetModel(
      (word_embedding): Embedding(32000, 768)
      (layer): ModuleList(
        (0): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (6): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (7): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (8): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (9): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (10): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (11): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layer): Classifier(
    (linear1): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2019-08-28 07:49:44,666 INFO] * number of parameters: 116719105
[2019-08-28 07:49:44,666 INFO] Start training...
[2019-08-28 07:49:44,817 INFO] Loading train dataset from ../xlnet_data/cnndm.train.51.xlnet.pt, number of examples: 1999
[2019-08-28 08:03:28,054 INFO] Device ID 0
[2019-08-28 08:03:28,054 INFO] Device cuda
[2019-08-28 08:03:34,999 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 08:03:35,000 INFO] Model config {
  "attn_type": "bi",
  "bi_data": false,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "n_head": 12,
  "n_layer": 12,
  "n_token": 32000,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "torchscript": false,
  "untie_r": true
}

[2019-08-28 08:03:35,354 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 08:03:40,742 INFO] Summarizer(
  (xlnet): XLNet(
    (model): XLNetModel(
      (word_embedding): Embedding(32000, 768)
      (layer): ModuleList(
        (0): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (6): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (7): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (8): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (9): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (10): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (11): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layer): Classifier(
    (linear1): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2019-08-28 08:03:40,746 INFO] * number of parameters: 116719105
[2019-08-28 08:03:40,746 INFO] Start training...
[2019-08-28 08:03:40,888 INFO] Loading train dataset from ../xlnet_data/cnndm.train.51.xlnet.pt, number of examples: 1999
[2019-08-28 08:08:12,578 INFO] Device ID 0
[2019-08-28 08:08:12,578 INFO] Device cuda
[2019-08-28 08:08:18,539 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 08:08:18,539 INFO] Model config {
  "attn_type": "bi",
  "bi_data": false,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "n_head": 12,
  "n_layer": 12,
  "n_token": 32000,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "torchscript": false,
  "untie_r": true
}

[2019-08-28 08:08:18,891 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 08:08:24,387 INFO] Summarizer(
  (xlnet): XLNet(
    (model): XLNetModel(
      (word_embedding): Embedding(32000, 768)
      (layer): ModuleList(
        (0): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (6): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (7): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (8): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (9): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (10): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (11): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (dropout): Dropout(p=0.1, inplace=False)
    )
  )
  (layer): Classifier(
    (linear1): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2019-08-28 08:08:24,391 INFO] * number of parameters: 116719105
[2019-08-28 08:08:24,391 INFO] Start training...
[2019-08-28 08:08:24,539 INFO] Loading train dataset from ../xlnet_data/cnndm.train.51.xlnet.pt, number of examples: 1999
[2019-08-28 08:36:48,103 INFO] Device ID 0
[2019-08-28 08:36:48,103 INFO] Device cuda
[2019-08-28 08:36:48,495 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 08:36:48,495 INFO] Model config {
  "attn_type": "bi",
  "bi_data": false,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "n_head": 12,
  "n_layer": 12,
  "n_token": 32000,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "torchscript": false,
  "untie_r": true
}

[2019-08-28 08:36:48,853 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 08:36:54,084 INFO] Summarizer(
  (xlnet): XLNet(
    (model): XLNetModel(
      (word_embedding): Embedding(32000, 768)
      (layer): ModuleList(
        (0): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (1): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (2): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (3): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (4): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (5): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (6): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (7): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (8): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (9): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (10): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (11): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
      )
      (dropout): Dropout(p=0.1)
    )
  )
  (layer): Classifier(
    (linear1): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2019-08-28 08:36:54,088 INFO] * number of parameters: 116719105
[2019-08-28 08:36:54,088 INFO] Start training...
[2019-08-28 08:36:54,206 INFO] Loading train dataset from ../xlnet_data/cnndm.train.51.xlnet.pt, number of examples: 1999
[2019-08-28 08:39:38,797 INFO] Device ID 0
[2019-08-28 08:39:38,797 INFO] Device cuda
[2019-08-28 08:39:39,192 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 08:39:39,192 INFO] Model config {
  "attn_type": "bi",
  "bi_data": false,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "n_head": 12,
  "n_layer": 12,
  "n_token": 32000,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "torchscript": false,
  "untie_r": true
}

[2019-08-28 08:39:39,543 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 08:39:44,691 INFO] Summarizer(
  (xlnet): XLNet(
    (model): XLNetModel(
      (word_embedding): Embedding(32000, 768)
      (layer): ModuleList(
        (0): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (1): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (2): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (3): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (4): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (5): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (6): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (7): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (8): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (9): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (10): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (11): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
      )
      (dropout): Dropout(p=0.1)
    )
  )
  (layer): Classifier(
    (linear1): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2019-08-28 08:39:44,696 INFO] * number of parameters: 116719105
[2019-08-28 08:39:44,696 INFO] Start training...
[2019-08-28 08:39:44,815 INFO] Loading train dataset from ../xlnet_data/cnndm.train.51.xlnet.pt, number of examples: 1999
[2019-08-28 08:55:23,019 INFO] Device ID 0
[2019-08-28 08:55:23,019 INFO] Device cuda
[2019-08-28 08:55:23,416 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 08:55:23,416 INFO] Model config {
  "attn_type": "bi",
  "bi_data": false,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "n_head": 12,
  "n_layer": 12,
  "n_token": 32000,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "torchscript": false,
  "untie_r": true
}

[2019-08-28 08:55:23,819 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 08:55:29,013 INFO] Summarizer(
  (xlnet): XLNet(
    (model): XLNetModel(
      (word_embedding): Embedding(32000, 768)
      (layer): ModuleList(
        (0): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (1): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (2): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (3): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (4): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (5): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (6): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (7): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (8): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (9): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (10): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (11): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
      )
      (dropout): Dropout(p=0.1)
    )
  )
  (layer): Classifier(
    (linear1): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2019-08-28 08:55:29,017 INFO] * number of parameters: 116719105
[2019-08-28 08:55:29,017 INFO] Start training...
[2019-08-28 08:55:29,142 INFO] Loading train dataset from ../xlnet_data/cnndm.train.51.xlnet.pt, number of examples: 1999
[2019-08-28 08:56:13,363 INFO] Device ID 0
[2019-08-28 08:56:13,364 INFO] Device cuda
[2019-08-28 08:56:13,747 INFO] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-config.json from cache at ../temp/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.ef1824921bc0786e97dc88d55eb17aabf18aac90f24bd34c0650529e7ba27d6f
[2019-08-28 08:56:13,748 INFO] Model config {
  "attn_type": "bi",
  "bi_data": false,
  "clamp_len": -1,
  "d_head": 64,
  "d_inner": 3072,
  "d_model": 768,
  "dropout": 0.1,
  "end_n_top": 5,
  "ff_activation": "gelu",
  "finetuning_task": null,
  "initializer_range": 0.02,
  "layer_norm_eps": 1e-12,
  "mem_len": null,
  "n_head": 12,
  "n_layer": 12,
  "n_token": 32000,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": false,
  "reuse_len": null,
  "same_length": false,
  "start_n_top": 5,
  "summary_activation": "tanh",
  "summary_last_dropout": 0.1,
  "summary_type": "last",
  "summary_use_proj": true,
  "torchscript": false,
  "untie_r": true
}

[2019-08-28 08:56:14,099 INFO] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/xlnet-base-cased-pytorch_model.bin from cache at ../temp/24197ba0ce5dbfe23924431610704c88e2c0371afa49149360e4c823219ab474.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac
[2019-08-28 08:56:19,367 INFO] Summarizer(
  (xlnet): XLNet(
    (model): XLNetModel(
      (word_embedding): Embedding(32000, 768)
      (layer): ModuleList(
        (0): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (1): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (2): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (3): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (4): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (5): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (6): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (7): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (8): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (9): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (10): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
        (11): XLNetLayer(
          (rel_attn): XLNetRelativeAttention(
            (layer_norm): XLNetLayerNorm()
            (dropout): Dropout(p=0.1)
          )
          (ff): XLNetFeedForward(
            (layer_norm): XLNetLayerNorm()
            (layer_1): Linear(in_features=768, out_features=3072, bias=True)
            (layer_2): Linear(in_features=3072, out_features=768, bias=True)
            (dropout): Dropout(p=0.1)
          )
          (dropout): Dropout(p=0.1)
        )
      )
      (dropout): Dropout(p=0.1)
    )
  )
  (layer): Classifier(
    (linear1): Linear(in_features=768, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
[2019-08-28 08:56:19,370 INFO] * number of parameters: 116719105
[2019-08-28 08:56:19,371 INFO] Start training...
[2019-08-28 08:56:19,494 INFO] Loading train dataset from ../xlnet_data/cnndm.train.51.xlnet.pt, number of examples: 1999
[2019-08-28 08:56:20,680 INFO] Step  1/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;      1 sec
[2019-08-28 08:56:22,561 INFO] Step  2/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;      3 sec
[2019-08-28 08:56:24,334 INFO] Step  3/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;      5 sec
[2019-08-28 08:56:24,957 INFO] Step  4/ 1000; xent: 0.00; lr: 1.0000000;   2 docs/s;      5 sec
[2019-08-28 08:56:26,422 INFO] Step  5/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;      7 sec
[2019-08-28 08:56:26,424 INFO] Saving checkpoint ../models/model_step_5.pt
[2019-08-28 08:56:29,488 INFO] Step  6/ 1000; xent: 0.00; lr: 1.0000000;   0 docs/s;     10 sec
[2019-08-28 08:56:29,769 INFO] Step  7/ 1000; xent: 0.00; lr: 1.0000000;   4 docs/s;     10 sec
[2019-08-28 08:56:31,015 INFO] Step  8/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     12 sec
[2019-08-28 08:56:32,259 INFO] Step  9/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     13 sec
[2019-08-28 08:56:32,539 INFO] Step 10/ 1000; xent: 0.00; lr: 1.0000000;   4 docs/s;     13 sec
[2019-08-28 08:56:32,540 INFO] Saving checkpoint ../models/model_step_10.pt
[2019-08-28 08:56:34,543 INFO] Step 11/ 1000; xent: 0.00; lr: 1.0000000;   0 docs/s;     15 sec
[2019-08-28 08:56:35,591 INFO] Step 12/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     16 sec
[2019-08-28 08:56:36,615 INFO] Step 13/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     17 sec
[2019-08-28 08:56:37,612 INFO] Step 14/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     18 sec
[2019-08-28 08:56:38,009 INFO] Step 15/ 1000; xent: 0.00; lr: 1.0000000;   3 docs/s;     19 sec
[2019-08-28 08:56:38,010 INFO] Saving checkpoint ../models/model_step_15.pt
[2019-08-28 08:56:41,380 INFO] Step 16/ 1000; xent: 0.00; lr: 1.0000000;   0 docs/s;     22 sec
[2019-08-28 08:56:42,514 INFO] Step 17/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     23 sec
[2019-08-28 08:56:44,834 INFO] Step 18/ 1000; xent: 0.00; lr: 1.0000000;   0 docs/s;     25 sec
[2019-08-28 08:56:46,014 INFO] Step 19/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     27 sec
[2019-08-28 08:56:46,511 INFO] Step 20/ 1000; xent: 0.00; lr: 1.0000000;   2 docs/s;     27 sec
[2019-08-28 08:56:46,513 INFO] Saving checkpoint ../models/model_step_20.pt
[2019-08-28 08:56:51,580 INFO] Step 21/ 1000; xent: 0.00; lr: 1.0000000;   0 docs/s;     32 sec
[2019-08-28 08:56:52,585 INFO] Step 22/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     33 sec
[2019-08-28 08:56:53,739 INFO] Step 23/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     34 sec
[2019-08-28 08:56:57,169 INFO] Step 24/ 1000; xent: 0.00; lr: 1.0000000;   1 docs/s;     38 sec
